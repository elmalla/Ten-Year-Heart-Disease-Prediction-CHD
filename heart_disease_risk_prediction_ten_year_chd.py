{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # **PREDICTING THE TEN YEAR RISK OF DEVELOPING HEART DISEASE USING MACHINE LEARNING**\n\n# %% [markdown]\n# ### ***1. Introduction*** \n# \n# Modfications on AMAYO II notebook\n# \n# Heart disease refers to various types of conditions that can affect heart function. These types include: coronary artery (atherosclerotic) disease that affects the blood supply to the heart, valvular heart disease that affects how the valves function to regulate blood flow, cardiomyopathies that affect heart muscles, heart rhythm disturbances (arrhythmias) that affect the electrical conduction and congenital heart diseases where the heart has structural defects that develop before birth.\n# \n# Heart disease is the major cause of morbidity and mortality globally: it accounts for more deaths annually than any other cause. For example an estimated 17.9 million people died from heart diseases in 2016, representing 31% of all global deaths. Over three quarters of these deaths took place in low- and middle-income countries.\n# \n# Most heart diseases are highly preventable and simple lifestyle modifications(such as reducing tobacco use, eating healthily, obesity and exercising) coupled with early treatment greately improve their prognoses. It is, however, difficult to identify high risk patients because of the mulfactorial nature of several contributory risk factors such as diabetes, high blood pressure, high cholesterol et cetera. Due to such constraints, scientists have turned towards modern approaches like Data Mining and Machine Learning for predicting the disease.\n# \n# Machine learning (ML), due to its superiority in pattern detection and classification, proves to be effective in assisting decision making and risk assesment from the large quantity of data produced by the healthcare industry on heart disease.\n# \n# In this notebook, I will be exploring different Machine Learning approaches for predicting wheather a patient has 10-year risk of developing coronary heart disease (CHD) using the [Framingham dataset ](http://biolincc.nhlbi.nih.gov/studies/framcohort/) that is publicly availabe on[ Kaggle ](https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset.)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:23.146782Z\",\"iopub.execute_input\":\"2022-07-05T04:15:23.147345Z\",\"iopub.status.idle\":\"2022-07-05T04:15:23.181229Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:23.147232Z\",\"shell.execute_reply\":\"2022-07-05T04:15:23.17968Z\"}}\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:23.183946Z\",\"iopub.execute_input\":\"2022-07-05T04:15:23.184464Z\",\"iopub.status.idle\":\"2022-07-05T04:15:23.947161Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:23.184417Z\",\"shell.execute_reply\":\"2022-07-05T04:15:23.946028Z\"}}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns\nfrom operator import add\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# %% [markdown]\n# ### 2. Dataset\n# The dataset is publically available on the [ Kaggle ](https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset.) website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients’ information. It includes over 4,000 records and 15 attributes. Variables Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n# \n# #### **Attributes**:\n# \n# ###### Demographic:\n# *     Sex: male or female(Nominal)\n# *     Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n# \n# ###### Education: no further information provided\n# \n# ###### Behavioral:\n# \n# *     Current Smoker: whether or not the patient is a current smoker (Nominal)\n# *     Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n#     \n# ###### Information on medical history:\n# \n# *     BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n# *     Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n# *     Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n# *     Diabetes: whether or not the patient had diabetes (Nominal)\n#     \n# ###### Information on current medical condition:\n# \n# *     Tot Chol: total cholesterol level (Continuous)\n# *     Sys BP: systolic blood pressure (Continuous)\n# *     Dia BP: diastolic blood pressure (Continuous)\n# *     BMI: Body Mass Index (Continuous)\n# *     Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n# *     Glucose: glucose level (Continuous)\n#     \n# #### **Target variable to predict:**\n# *10 year risk of coronary heart disease (CHD) - (binary: “1”, means “Yes”, “0” means “No”)*\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:23.948838Z\",\"iopub.execute_input\":\"2022-07-05T04:15:23.949735Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.012256Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:23.949697Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.011132Z\"}}\n#load the data\ndata = pd.read_csv('../input/heart-diseases/datasets_4123_6408_framingham.csv')\ndata.drop(['education'],axis=1,inplace=True)\ndata.head()\n\n# %% [markdown]\n# We drop the education column because it has no correlation with heart disease\n\n# %% [markdown]\n# ### **3. Exploratory Data Analysis**\n\n# %% [markdown]\n# #### 3.1 Missing variables\n# \n# Handling missing data is important as many machine learning algorithms do not support data with missing values.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.013716Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.0143Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.022637Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.014265Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.021551Z\"}}\n#total percentage of missing data\nmissing_data = data.isnull().sum()\ntotal_percentage = (missing_data.sum()/data.shape[0]) * 100\nprint(f'The total percentage of missing data is {round(total_percentage,2)}%')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.025174Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.025541Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.054361Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.025509Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.053447Z\"}}\n# percentage of missing data per category\ntotal = data.isnull().sum().sort_values(ascending=False)\npercent_total = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)*100\nmissing = pd.concat([total, percent_total], axis=1, keys=[\"Total\", \"Percentage\"])\nmissing_data = missing[missing['Total']>0]\nmissing_data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.05574Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.056271Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.350564Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.056237Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.349267Z\"}}\nplt.figure(figsize=(9,6))\nsns.set(style=\"whitegrid\")\nsns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)\nplt.title('Percentage of missing data by feature')\nplt.xlabel('Features', fontsize=14)\nplt.ylabel('Percentage', fontsize=14)\nplt.show()\n\n# %% [markdown]\n# At 9.15%, the blood glucose entry has the highest percentage of missing data. The otherfeatures have very few missing entries.\n# \n# Since the missing entries account for only 12% of the total data we can drop these entries without losing alot of data.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.35216Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.353441Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.363998Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.353399Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.362849Z\"}}\n# drop missing entries\ndata.dropna(axis=0, inplace=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.36577Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.366863Z\",\"iopub.status.idle\":\"2022-07-05T04:15:24.376201Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.366789Z\",\"shell.execute_reply\":\"2022-07-05T04:15:24.374918Z\"}}\ndata.shape\n\n# %% [markdown]\n# #### 3.2 Data Distribution\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:24.377599Z\",\"iopub.execute_input\":\"2022-07-05T04:15:24.380227Z\",\"iopub.status.idle\":\"2022-07-05T04:15:27.537433Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:24.380168Z\",\"shell.execute_reply\":\"2022-07-05T04:15:27.536098Z\"}}\n# plot histogram to see the distribution of the data\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndata.hist(ax = ax)\nplt.show()\n\n# %% [markdown]\n# The data on the prevalent stroke, diabetes, and blood pressure meds are poorly balanced\n\n# %% [markdown]\n# #### 3.2.1 Case counts\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:27.539567Z\",\"iopub.execute_input\":\"2022-07-05T04:15:27.540382Z\",\"iopub.status.idle\":\"2022-07-05T04:15:27.730366Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:27.54033Z\",\"shell.execute_reply\":\"2022-07-05T04:15:27.729095Z\"}}\nsns.countplot(x='TenYearCHD',data=data)\nplt.show()\ncases = data.TenYearCHD.value_counts()\nprint(f\"There are {cases[0]} patients without heart disease and {cases[1]} patients with the disease\")\n\n# %% [markdown]\n# The data is not properly balanced as the number of people without the disease greately exceeds the number of people with the disease. The ratio is about 1:5.57\n\n# %% [markdown]\n# #### 3.3 Categorical variable comparisons\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:27.73527Z\",\"iopub.execute_input\":\"2022-07-05T04:15:27.735645Z\",\"iopub.status.idle\":\"2022-07-05T04:15:27.754991Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:27.735614Z\",\"shell.execute_reply\":\"2022-07-05T04:15:27.753396Z\"}}\ndef stacked_barchart(data, title = None, ylabel = None, xlabel = None):\n    default_colors = ['#008080', '#5f3c41', '#219AD8']\n    # From raw value to percentage\n    totals = data.sum(axis=1)\n    bars = ((data.T / totals) * 100).T\n    r = list(range(data.index.size))\n\n    # Plot\n    barWidth = 0.95\n    names = data.index.tolist()\n    bottom = [0] * bars.shape[0]\n\n    # Create bars\n    color_index = 0\n    plots = []\n    for bar in bars.columns:\n        plots.append(plt.bar(r, bars[bar], bottom=bottom, color=default_colors[color_index], edgecolor='white', width=barWidth))\n        bottom = list(map(add, bottom, bars[bar]))\n        color_index = 0 if color_index >= len(default_colors) else color_index + 1\n\n    # Custom x axis\n    plt.title(title)\n    plt.xticks(r, names)\n    plt.xlabel(data.index.name if xlabel is None else xlabel)\n    plt.ylabel(data.columns.name if ylabel is None else ylabel)\n    ax = plt.gca()\n        \n    y_labels = ax.get_yticks()\n    ax.set_yticklabels([str(y) + '%' for y in y_labels])\n\n    flat_list = [item for sublist in data.T.values for item in sublist]\n    for i, d in zip(ax.patches, flat_list):\n        data_label = str(d) + \" (\" + str(round(i.get_height(), 2)) + \"%)\"\n        ax.text(i.get_x() + 0.45, i.get_y() + 5, data_label, horizontalalignment='center', verticalalignment='center', fontdict = dict(color = 'white', size = 20))\n\n    for item in ([ax.title]):\n        item.set_fontsize(27)\n        \n    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(24)\n    \n    legend = ax.legend(plots, bars.columns.tolist(), fancybox=True)\n    plt.setp(legend.get_texts(), fontsize='20')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:27.757126Z\",\"iopub.execute_input\":\"2022-07-05T04:15:27.757643Z\",\"iopub.status.idle\":\"2022-07-05T04:15:28.904162Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:27.757583Z\",\"shell.execute_reply\":\"2022-07-05T04:15:28.902197Z\"}}\nfig = plt.gcf()\nfig.set_size_inches(25, 35)\ngrid_rows = 3\ngrid_cols = 2\n\n#draw sex vs disease outcome\nplt.subplot(grid_rows, grid_cols, 1)\ntemp = data[['male','TenYearCHD']].groupby(['male','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Female', 1:'Male'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Sex', ylabel = 'Population')\n\n#draw smoking satus vs disease outcome\nplt.subplot(grid_rows, grid_cols, 2)\ntemp = data[['currentSmoker','TenYearCHD']].groupby(['currentSmoker','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not a Smoker', 1:'Smoker'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Smoking', ylabel = 'Population')\n\n#draw diabetes vs disease outcome\nplt.subplot(grid_rows, grid_cols, 3)\ntemp = data[['diabetes','TenYearCHD']].groupby(['diabetes','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not Diabetic', 1:'Diabetic'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Diabetes', ylabel = 'Population')\n\n#draw BP meds vs disease outcome\nplt.subplot(grid_rows, grid_cols, 4)\ntemp = data[['BPMeds','TenYearCHD']].groupby(['BPMeds','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not on medication', 1:'On Medication'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs BP meds', ylabel = 'Population')\n\n#draw Hypertension vs disease outcome\nplt.subplot(grid_rows, grid_cols, 5)\ntemp = data[['prevalentHyp','TenYearCHD']].groupby(['prevalentHyp','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not Hypertensive', 1:'Hypertensive'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Hypertension', ylabel = 'Population')\n\n# %% [markdown]\n# Due to the imbalanced nature of the dataset it is difficult to make conclusions but based on what is observed but these are the conclusions that can be drawn:\n# \n# *      Slightly more males are suffering from CHD than females\n# *      The percentage of people who have CHD is almost equal between smokers and non smokers\n# *      The percentage of people who have CHD is higher among the diabetic, and those with prevalent hypertesion as compared to those who dont have similar morbidities\n# *      A larger percentage of the people who have CHD are on blood pressure medication\n\n# %% [markdown]\n# #### 3.4 Number of people who have disease vs age\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:28.90601Z\",\"iopub.execute_input\":\"2022-07-05T04:15:28.906399Z\",\"iopub.status.idle\":\"2022-07-05T04:15:29.492405Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:28.906367Z\",\"shell.execute_reply\":\"2022-07-05T04:15:29.49106Z\"}}\npositive_cases = data[data['TenYearCHD'] == 1]\nplt.figure(figsize=(15,6))\nsns.countplot(x='age',data = positive_cases, hue = 'TenYearCHD', palette='husl')\nplt.show()\n\n# %% [markdown]\n# The people with the highest risk of developing CHD are betwwen the ages of 51 and 63\n# \n# The number of sick people generally increases with age\n\n# %% [markdown]\n# #### 3.5 Correlation Heat map\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:29.493711Z\",\"iopub.execute_input\":\"2022-07-05T04:15:29.494088Z\",\"iopub.status.idle\":\"2022-07-05T04:15:30.816253Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:29.494048Z\",\"shell.execute_reply\":\"2022-07-05T04:15:30.815Z\"}}\nplt.figure(figsize=(15,8))\nsns.heatmap(data.corr(), annot = True)\nplt.show()\n\n# %% [markdown]\n# There are no features with more than 0.5 correlation with the Ten year risk of developing CHD and this shows that the features a poor predictors. However the features with the highest correlations are age, prevalent hypertension and systolic blood pressure\n# \n# Also there are a couple of features that are highly correlated with one another and it makes no sense to use both of them in building a machine learning model. These incluse: Blood glucose and diabetes (obviously); systolic and diastolic blood pressures; cigarette smoking and the number of cigarretes smoked per day. Therefore we need to carry out feature selection to pick the best features\n\n# %% [markdown]\n# ### **4 Feature Selection**\n\n# %% [markdown]\n# Here we will use the Boruta algorithm  which is a wrapper built around the random forest classification algorithm. It tries to capture all the important, interesting features in a data set with respect to an outcome variable.\n# \n# ##### Methodology:\n# \n# *     Firstly, it adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features).\n#     \n# *     Then, it trains a random forest classifier on the extended data set and applies a feature importance measure (the default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important.\n#     \n# *     At every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z-score than the maximum Z-score of its shadow features) and constantly removes features which are deemed highly unimportant.\n#     \n# *     Finally, the algorithm stops either when all features get confirmed or rejected or it reaches a specified limit of random forest runs.\n#     \n# Check the full reference [here]( http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:30.818087Z\",\"iopub.execute_input\":\"2022-07-05T04:15:30.818421Z\",\"iopub.status.idle\":\"2022-07-05T04:15:30.984552Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:30.818391Z\",\"shell.execute_reply\":\"2022-07-05T04:15:30.983334Z\"}}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:15:30.986112Z\",\"iopub.execute_input\":\"2022-07-05T04:15:30.986468Z\",\"iopub.status.idle\":\"2022-07-05T04:16:04.066763Z\",\"shell.execute_reply.started\":\"2022-07-05T04:15:30.986436Z\",\"shell.execute_reply\":\"2022-07-05T04:16:04.065485Z\"}}\n#define the features\nX = data.iloc[:,:-1].values\ny = data.iloc[:,-1].values\n\nforest = RandomForestClassifier(n_estimators=1000, n_jobs=-1, class_weight='balanced')\n\n# define Boruta feature selection method\nfeat_selector = BorutaPy(forest, n_estimators='auto', verbose=2)\n \n# find all relevant features\nfeat_selector.fit(X, y)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:04.068704Z\",\"iopub.execute_input\":\"2022-07-05T04:16:04.069491Z\",\"iopub.status.idle\":\"2022-07-05T04:16:04.080706Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:04.069443Z\",\"shell.execute_reply\":\"2022-07-05T04:16:04.078608Z\"}}\n# show the most important features\nmost_important = data.columns[:-1][feat_selector.support_].tolist()\nmost_important\n\n# %% [markdown]\n# We see that age and systolic blood pressures are selected as the most important features for predicting the Ten year risk of developing CHD.\n# \n# However we will use the six most important features to build our models\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:04.083085Z\",\"iopub.execute_input\":\"2022-07-05T04:16:04.083827Z\",\"iopub.status.idle\":\"2022-07-05T04:16:04.096642Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:04.083756Z\",\"shell.execute_reply\":\"2022-07-05T04:16:04.095675Z\"}}\n# select the top 6 features\ntop_features = data.columns[:-1][feat_selector.ranking_ <=6].tolist()\ntop_features\n\n# %% [markdown]\n# The top features are:\n#     1. Age\n#     2. Total cholesterol\n#     3. Systolic blood pressure\n#     4. Diastolic blood pressure\n#     5. BMI\n#     6. Heart rate\n#     7. Blood glucose\n\n# %% [markdown]\n# #### 4.1 Statistics on the top features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:04.098144Z\",\"iopub.execute_input\":\"2022-07-05T04:16:04.098479Z\",\"iopub.status.idle\":\"2022-07-05T04:16:05.299645Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:04.098449Z\",\"shell.execute_reply\":\"2022-07-05T04:16:05.297816Z\"}}\nimport statsmodels.api as sm\n\n# %% [markdown]\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:05.301474Z\",\"iopub.execute_input\":\"2022-07-05T04:16:05.301865Z\",\"iopub.status.idle\":\"2022-07-05T04:16:05.310276Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:05.301824Z\",\"shell.execute_reply\":\"2022-07-05T04:16:05.308526Z\"}}\nX_top = data[top_features]\ny = data['TenYearCHD']\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:05.314184Z\",\"iopub.execute_input\":\"2022-07-05T04:16:05.31491Z\",\"iopub.status.idle\":\"2022-07-05T04:16:05.485566Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:05.314845Z\",\"shell.execute_reply\":\"2022-07-05T04:16:05.48411Z\"}}\nres = sm.Logit(y,X_top).fit()\nres.summary()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:05.487876Z\",\"iopub.execute_input\":\"2022-07-05T04:16:05.489395Z\",\"iopub.status.idle\":\"2022-07-05T04:16:05.517404Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:05.489332Z\",\"shell.execute_reply\":\"2022-07-05T04:16:05.515907Z\"}}\nparams = res.params\nconf = res.conf_int()\nconf['Odds Ratio'] = params\nconf.columns = ['5%', '95%', 'Odds Ratio']\nprint(np.exp(conf))\n\n# %% [markdown]\n# Holding all other features constant, the odds of getting diagnosed with heart disease increases with about 2% for every increase in age an systolic blood pressure\n# \n# The other factors show no significant positive odds\n\n# %% [markdown]\n# #### 4.2 Pair plots\n\n# %% [markdown]\n# #inital pallete number was 10 which showed an error we used the solution from : https://stackoverflow.com/questions/56131511/how-to-set-seaborn-color-palette-for-multiple-categories\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:05.526577Z\",\"iopub.execute_input\":\"2022-07-05T04:16:05.527849Z\",\"iopub.status.idle\":\"2022-07-05T04:16:26.609835Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:05.527765Z\",\"shell.execute_reply\":\"2022-07-05T04:16:26.607768Z\"}}\nsns.pairplot(data, hue = 'TenYearCHD', markers=[\"o\", \"s\"], vars = top_features, palette = sns.color_palette(\"bright\", data.TenYearCHD.unique().shape[0]))\n\n# %% [markdown]\n# There are no characteristics that can be used split the data well\n\n# %% [markdown]\n# ### **5. Models and predictions**\n\n# %% [markdown]\n# Since the dataset is imbalanced i.e for every positive case there are about 6 negative cases. We may end up with a classifier that is biased to the negative cases. The classifier may have a high accuracy but poor a precision and recall. To adress this we will balance the dataset using the Synthetic Minority Oversampling Technique (SMOTE).\n\n# %% [markdown]\n# #### 5.1 SMOTE\n\n# %% [markdown]\n# > *SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n# *\n# — Page 47, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n# \n# This procedure can be used to create as many synthetic examples for the minority class as are required. It suggests first using random undersampling to trim the number of examples in the majority class, then use SMOTE to oversample the minority class to balance the class distribution.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:26.611896Z\",\"iopub.execute_input\":\"2022-07-05T04:16:26.612604Z\",\"iopub.status.idle\":\"2022-07-05T04:16:26.784211Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:26.612567Z\",\"shell.execute_reply\":\"2022-07-05T04:16:26.782896Z\"}}\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:26.78591Z\",\"iopub.execute_input\":\"2022-07-05T04:16:26.786282Z\",\"iopub.status.idle\":\"2022-07-05T04:16:26.794566Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:26.786249Z\",\"shell.execute_reply\":\"2022-07-05T04:16:26.793031Z\"}}\nX = data[top_features]\ny = data.iloc[:,-1]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:26.796632Z\",\"iopub.execute_input\":\"2022-07-05T04:16:26.797102Z\",\"iopub.status.idle\":\"2022-07-05T04:16:26.833817Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:26.797065Z\",\"shell.execute_reply\":\"2022-07-05T04:16:26.8329Z\"}}\n# the numbers before SMOTE\nnum_before = dict(Counter(y))\n\n#perform SMOTE\n\n# define pipeline\nover = SMOTE(sampling_strategy=0.8)\nunder = RandomUnderSampler(sampling_strategy=0.8)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\n# transform the dataset\nX_smote, y_smote = pipeline.fit_resample(X, y)\n\n\n#the numbers after SMOTE\nnum_after =dict(Counter(y_smote))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:26.835528Z\",\"iopub.execute_input\":\"2022-07-05T04:16:26.835999Z\",\"iopub.status.idle\":\"2022-07-05T04:16:26.843043Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:26.835955Z\",\"shell.execute_reply\":\"2022-07-05T04:16:26.841985Z\"}}\nprint(num_before, num_after)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:26.844952Z\",\"iopub.execute_input\":\"2022-07-05T04:16:26.845292Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.178589Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:26.845263Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.177651Z\"}}\nlabels = [\"Negative Cases\",\"Positive Cases\"]\nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\nsns.barplot(labels, list(num_before.values()))\nplt.title(\"Numbers Before Balancing\")\nplt.subplot(1,2,2)\nsns.barplot(labels, list(num_after.values()))\nplt.title(\"Numbers After Balancing\")\nplt.show()\n\n# %% [markdown]\n# After applying SMOTE, the new dataset is much more balanced: the new ratio between negative and positive cases is 1:1.2 up from 1:5.57\n\n# %% [markdown]\n# #### 5.2 Splitting data to Training and Testing set \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.188601Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.189019Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.196858Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.188984Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.19564Z\"}}\n#if len(top_features)>=9:\n#    mycolumn_names = ['age', 'cigsPerDay','totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose','TenYearCHD']\n#else:\n#    mycolumn_names = ['age', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose','TenYearCHD']\n\nmycolumn_names = top_features + ['TenYearCHD'] \nprint('top_features :',top_features)\nprint('top_features count :',len(top_features))\nprint ('mycolumn_names ', mycolumn_names )\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:12:16.697387Z\",\"iopub.execute_input\":\"2022-07-05T05:12:16.698572Z\",\"iopub.status.idle\":\"2022-07-05T05:12:16.724113Z\",\"shell.execute_reply.started\":\"2022-07-05T05:12:16.698525Z\",\"shell.execute_reply\":\"2022-07-05T05:12:16.722359Z\"}}\nX_smote.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.199041Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.19974Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.225857Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.199682Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.224655Z\"}}\n# new dataset\nnew_data = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(y_smote)], axis=1)\n#new_data.columns = ['age', 'cigsPerDay','totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose','TenYearCHD']\nnew_data.columns = mycolumn_names\nnew_data.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.227873Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.228353Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.248688Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.22831Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.247889Z\"}}\nX_new = new_data[top_features]\ny_new= new_data.iloc[:,-1]\nX_new.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.250112Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.250441Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.25941Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.250411Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.25844Z\"}}\n# split the dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_new,y_new,test_size=.2,random_state=42)\n\n# %% [markdown]\n# #### 5.3 Feature Scaling\n# \n# Feature scaling is a method used to normalize the range of independent variables or features of data. It generally speeds up the running time of different algorithms\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.261247Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.262232Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.271783Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.262183Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.270637Z\"}}\nfrom sklearn.preprocessing import StandardScaler\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.273716Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.274605Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.292354Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.274545Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.291233Z\"}}\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_scaled)\n\nX_test_scaled = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test_scaled)\n\n# %% [markdown]\n# #### save scaler\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:30:36.279904Z\",\"iopub.execute_input\":\"2022-07-05T05:30:36.280443Z\",\"iopub.status.idle\":\"2022-07-05T05:30:36.291439Z\",\"shell.execute_reply.started\":\"2022-07-05T05:30:36.280404Z\",\"shell.execute_reply\":\"2022-07-05T05:30:36.290193Z\"}}\n#dump(scaler, 'std_scaler.bin', compress=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.293749Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.294699Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.311081Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.294663Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.309753Z\"}}\nX_train.head()\n\n# %% [markdown]\n# #### 5.4 Models\n\n# %% [markdown]\n# The four algorithms that will be used are:\n# 1. Logistic Regression\n# 2. k-Nearest Neighbours\n# 3. Decision Trees\n# 4. Support Vector Machine\n\n# %% [markdown]\n# ###### 5.4.1 Logistic regression\n\n# %% [markdown]\n# The goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest:\n# ![logit function](http://www.statisticssolutions.com/wp-content/uploads/2010/01/log23.jpg)\n# \n# Rather than choosing parameters that minimize the sum of squared errors (like in ordinary regression), estimation in logistic regression chooses parameters that maximize the likelihood of observing the sample values.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.315633Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.318235Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.326541Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.318132Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.325493Z\"}}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.327794Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.328924Z\",\"iopub.status.idle\":\"2022-07-05T04:16:27.340911Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.328885Z\",\"shell.execute_reply\":\"2022-07-05T04:16:27.339936Z\"}}\n# search for optimun parameters using gridsearch\nparams = {'penalty':['l1','l2'],\n         'C':[0.01,0.1,1,10,100],\n         'class_weight':['balanced',None]}\nlogistic_clf = GridSearchCV(LogisticRegression(),param_grid=params,cv=10)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:27.342108Z\",\"iopub.execute_input\":\"2022-07-05T04:16:27.342988Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.080303Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:27.34294Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.078955Z\"}}\n#train the classifier\nlogistic_clf.fit(X_train,y_train)\n\nlogistic_clf.best_params_\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.087202Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.090931Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.103854Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.090856Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.101776Z\"}}\n#make predictions\nlogistic_predict = logistic_clf.predict(X_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.111929Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.116243Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.132062Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.116152Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.130642Z\"}}\nlog_accuracy = accuracy_score(y_test,logistic_predict)\nprint(f\"Using logistic regression we get an accuracy of {round(log_accuracy*100,2)}%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.139421Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.140673Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.425086Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.140614Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.423737Z\"}}\ncm=confusion_matrix(y_test,logistic_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.427671Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.428195Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.44342Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.428148Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.442211Z\"}}\nprint(classification_report(y_test,logistic_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.444936Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.445705Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.457101Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.445645Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.455572Z\"}}\nlogistic_f1 = f1_score(y_test, logistic_predict)\nprint(f'The f1 score for logistic regression is {round(logistic_f1*100,2)}%')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.45895Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.459427Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.738102Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.459393Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.73685Z\"}}\n# ROC curve and AUC \nprobs = logistic_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nlog_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(log_auc,3)}\")\nplt.show()\n\n# %% [markdown]\n# ##### 5.4.2 k-Nearest Neighbours\n\n# %% [markdown]\n# The k-nearest-neighbors is a data classification algorithm that attempts to determine what group a data point is in by looking at the data points around it.\n# \n# An algorithm, looking at one point on a grid, trying to determine if a point is in group A or B, looks at the states of the points that are near it. The range is arbitrarily determined, but the point is to take a sample of the data. If the majority of the points are in group A, then it is likely that the data point in question will be A rather than B, and vice versa.\n# \n# The k-nearest-neighbor is an example of a \"lazy learner\" algorithm because it does not generate a model of the data set beforehand. The only calculations it makes are when it is asked to poll the data point's neighbors. This makes k-nn very easy to implement for data mining.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.739887Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.74059Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.746438Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.740545Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.74489Z\"}}\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.749338Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.750113Z\",\"iopub.status.idle\":\"2022-07-05T04:16:30.760562Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.750054Z\",\"shell.execute_reply\":\"2022-07-05T04:16:30.759314Z\"}}\n# search for optimun parameters using gridsearch\nparams= {'n_neighbors': np.arange(1, 10)}\ngrid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = params, \n                           scoring = 'accuracy', cv = 10, n_jobs = -1)\nknn_clf = GridSearchCV(KNeighborsClassifier(),params,cv=3, n_jobs=-1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:30.762327Z\",\"iopub.execute_input\":\"2022-07-05T04:16:30.763113Z\",\"iopub.status.idle\":\"2022-07-05T04:16:33.723582Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:30.763069Z\",\"shell.execute_reply\":\"2022-07-05T04:16:33.722594Z\"}}\n# train the model\nknn_clf.fit(X_train,y_train)\nknn_clf.best_params_ \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:33.725016Z\",\"iopub.execute_input\":\"2022-07-05T04:16:33.726246Z\",\"iopub.status.idle\":\"2022-07-05T04:16:33.798959Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:33.726201Z\",\"shell.execute_reply\":\"2022-07-05T04:16:33.797539Z\"}}\n# predictions\nknn_predict = knn_clf.predict(X_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:33.801151Z\",\"iopub.execute_input\":\"2022-07-05T04:16:33.801519Z\",\"iopub.status.idle\":\"2022-07-05T04:16:33.810562Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:33.801489Z\",\"shell.execute_reply\":\"2022-07-05T04:16:33.809077Z\"}}\n#accuracy\nknn_accuracy = accuracy_score(y_test,knn_predict)\nprint(f\"Using k-nearest neighbours we get an accuracy of {round(knn_accuracy*100,2)}%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:33.81224Z\",\"iopub.execute_input\":\"2022-07-05T04:16:33.812895Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.078703Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:33.81285Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.077683Z\"}}\ncm=confusion_matrix(y_test,knn_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.08012Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.080769Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.095662Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.080733Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.094182Z\"}}\nprint(classification_report(y_test,knn_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.097079Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.097637Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.106062Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.097604Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.104871Z\"}}\nknn_f1 = f1_score(y_test, knn_predict)\nprint(f'The f1 score for K nearest neignbours is {round(knn_f1*100,2)}%')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.108211Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.10887Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.416172Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.108828Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.415101Z\"}}\n# ROC curve and AUC \nprobs = knn_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nknn_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(knn_auc,3)}\")\nplt.show()\n\n# %% [markdown]\n# #### 5.4.3 Decision Trees\n\n# %% [markdown]\n# A decision tree is a tree-like graph with nodes representing the place where we pick an attribute and ask a question; edges represent the answers the to the question; and the leaves represent the actual output or class label. They are used in non-linear decision making with simple linear decision surface.\n# \n# Decision trees classify the examples by sorting them down the tree from the root to some leaf node, with the leaf node providing the classification to the example. Each node in the tree acts as a test case for some attribute, and each edge descending from that node corresponds to one of the possible answers to the test case. This process is recursive in nature and is repeated for every subtree rooted at the new nodes.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.417655Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.41802Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.423026Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.417987Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.421988Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\ndtree= DecisionTreeClassifier(random_state=7)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.424417Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.424921Z\",\"iopub.status.idle\":\"2022-07-05T04:16:34.437286Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.424886Z\",\"shell.execute_reply\":\"2022-07-05T04:16:34.435902Z\"}}\n# grid search for optimum parameters\nparams = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\ntree_clf = GridSearchCV(dtree, param_grid=params, n_jobs=-1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:34.441064Z\",\"iopub.execute_input\":\"2022-07-05T04:16:34.442047Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.447684Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:34.441885Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.446441Z\"}}\n# train the model\ntree_clf.fit(X_train,y_train)\ntree_clf.best_params_ \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.44954Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.4503Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.458765Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.450252Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.457716Z\"}}\n# predictions\ntree_predict = tree_clf.predict(X_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.460612Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.461158Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.474946Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.461111Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.473733Z\"}}\n#accuracy\ntree_accuracy = accuracy_score(y_test,tree_predict)\nprint(f\"Using Decision Trees we get an accuracy of {round(tree_accuracy*100,2)}%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.476502Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.477338Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.736283Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.477289Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.734761Z\"}}\ncm=confusion_matrix(y_test,tree_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.73829Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.738731Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.755084Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.738619Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.753195Z\"}}\nprint(classification_report(y_test,tree_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.759687Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.760323Z\",\"iopub.status.idle\":\"2022-07-05T04:16:46.771254Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.76028Z\",\"shell.execute_reply\":\"2022-07-05T04:16:46.76953Z\"}}\ntree_f1 = f1_score(y_test, tree_predict)\nprint(f'The f1 score Descision trees is {round(tree_f1*100,2)}%')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:46.774831Z\",\"iopub.execute_input\":\"2022-07-05T04:16:46.775862Z\",\"iopub.status.idle\":\"2022-07-05T04:16:47.080141Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:46.775792Z\",\"shell.execute_reply\":\"2022-07-05T04:16:47.078919Z\"}}\n# ROC curve and AUC \nprobs = tree_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntree_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(tree_auc,3)}\")\nplt.show()\n\n# %% [markdown]\n# #### 5.4.4 Support Vector Machine\n\n# %% [markdown]\n# A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:47.081939Z\",\"iopub.execute_input\":\"2022-07-05T04:16:47.082917Z\",\"iopub.status.idle\":\"2022-07-05T04:16:47.089522Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:47.082875Z\",\"shell.execute_reply\":\"2022-07-05T04:16:47.087047Z\"}}\nfrom sklearn.svm import SVC\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:47.091198Z\",\"iopub.execute_input\":\"2022-07-05T04:16:47.091614Z\",\"iopub.status.idle\":\"2022-07-05T04:16:47.105897Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:47.091579Z\",\"shell.execute_reply\":\"2022-07-05T04:16:47.103994Z\"}}\n#grid search for optimum parameters\nCs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\nsvm_clf = GridSearchCV(SVC(kernel='rbf', probability=True), param_grid, cv=10)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:16:47.108603Z\",\"iopub.execute_input\":\"2022-07-05T04:16:47.109483Z\",\"iopub.status.idle\":\"2022-07-05T04:31:11.774723Z\",\"shell.execute_reply.started\":\"2022-07-05T04:16:47.109382Z\",\"shell.execute_reply\":\"2022-07-05T04:31:11.773559Z\"}}\n# train the model\nsvm_clf.fit(X_train,y_train)\nsvm_clf.best_params_ \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:11.776254Z\",\"iopub.execute_input\":\"2022-07-05T04:31:11.776862Z\",\"iopub.status.idle\":\"2022-07-05T04:31:11.939449Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:11.776814Z\",\"shell.execute_reply\":\"2022-07-05T04:31:11.938235Z\"}}\n# predictions\nsvm_predict = svm_clf.predict(X_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:11.940975Z\",\"iopub.execute_input\":\"2022-07-05T04:31:11.941531Z\",\"iopub.status.idle\":\"2022-07-05T04:31:11.948718Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:11.941498Z\",\"shell.execute_reply\":\"2022-07-05T04:31:11.947372Z\"}}\n#accuracy\nsvm_accuracy = accuracy_score(y_test,svm_predict)\nprint(f\"Using SVM we get an accuracy of {round(svm_accuracy*100,2)}%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:11.950345Z\",\"iopub.execute_input\":\"2022-07-05T04:31:11.950886Z\",\"iopub.status.idle\":\"2022-07-05T04:31:12.222855Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:11.95071Z\",\"shell.execute_reply\":\"2022-07-05T04:31:12.221904Z\"}}\ncm=confusion_matrix(y_test,svm_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:12.224323Z\",\"iopub.execute_input\":\"2022-07-05T04:31:12.224935Z\",\"iopub.status.idle\":\"2022-07-05T04:31:12.238694Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:12.224902Z\",\"shell.execute_reply\":\"2022-07-05T04:31:12.236823Z\"}}\nprint(classification_report(y_test,svm_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:12.240224Z\",\"iopub.execute_input\":\"2022-07-05T04:31:12.241007Z\",\"iopub.status.idle\":\"2022-07-05T04:31:12.250367Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:12.240973Z\",\"shell.execute_reply\":\"2022-07-05T04:31:12.249424Z\"}}\nsvm_f1 = f1_score(y_test, svm_predict)\nprint(f'The f1 score for SVM is {round(svm_f1*100,2)}%')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:12.252152Z\",\"iopub.execute_input\":\"2022-07-05T04:31:12.253275Z\",\"iopub.status.idle\":\"2022-07-05T04:31:12.716296Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:12.253227Z\",\"shell.execute_reply\":\"2022-07-05T04:31:12.714399Z\"}}\n# ROC curve and AUC \nprobs = svm_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nsvm_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(svm_auc,3)}\")\nplt.show()\n\n# %% [code]\n\n\n# %% [markdown]\n# #### The code below added from PRATIBHA BOTHRA notebook which have prediction issues\n\n# %% [markdown]\n# ### Random Forest\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:12.718337Z\",\"iopub.execute_input\":\"2022-07-05T04:31:12.719404Z\",\"iopub.status.idle\":\"2022-07-05T04:31:14.378111Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:12.719358Z\",\"shell.execute_reply\":\"2022-07-05T04:31:14.376835Z\"}}\n#Evaluation And Accuracy\nm4 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth=12)\nrf.fit(X_train,y_train)\nrf_predict = rf.predict(X_test)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(y_test, rf_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:14.37939Z\",\"iopub.execute_input\":\"2022-07-05T04:31:14.379727Z\",\"iopub.status.idle\":\"2022-07-05T04:31:14.668184Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:14.379699Z\",\"shell.execute_reply\":\"2022-07-05T04:31:14.666894Z\"}}\n#Confusion Matrix\ncm = confusion_matrix(y_test, rf_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Greens\") \nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(y_test, rf_predict)) \n\n# %% [markdown]\n# ### Gradient Boosting Classifier\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:14.669819Z\",\"iopub.execute_input\":\"2022-07-05T04:31:14.670188Z\",\"iopub.status.idle\":\"2022-07-05T04:31:15.528561Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:14.670156Z\",\"shell.execute_reply\":\"2022-07-05T04:31:15.527242Z\"}}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nm5 = 'Gradient Boosting Classifier'\ngbc =  GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\ngbc_predict = gbc.predict(X_test)\n\nfrom sklearn.metrics import jaccard_score \nprint('Accuracy of the model in jaccard similarity score is = ',  \n      jaccard_score(y_test, gbc_predict))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:15.530686Z\",\"iopub.execute_input\":\"2022-07-05T04:31:15.531064Z\",\"iopub.status.idle\":\"2022-07-05T04:31:16.187083Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:15.531032Z\",\"shell.execute_reply\":\"2022-07-05T04:31:16.185886Z\"}}\n#Confusion Matrix\ncm = confusion_matrix(y_test, gbc_predict) \nconf_matrix = pd.DataFrame(data = cm,  \n                           columns = ['Predicted:0', 'Predicted:1'],  \n                           index =['Actual:0', 'Actual:1']) \nplt.figure(figsize = (8, 5)) \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Greens_r\")\nplt.show() \n  \nprint('The details for confusion matrix is =') \nprint (classification_report(y_test, gbc_predict))\n\n# %% [markdown]\n# #### 5.5 Model Comparison\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:16.18864Z\",\"iopub.execute_input\":\"2022-07-05T04:31:16.189008Z\",\"iopub.status.idle\":\"2022-07-05T04:31:16.198751Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:16.188975Z\",\"shell.execute_reply\":\"2022-07-05T04:31:16.197273Z\"}}\ncomparison = pd.DataFrame({\n    \"Logistic regression\":{'Accuracy':log_accuracy, 'AUC':log_auc, 'F1 score':logistic_f1},\n    \"K-nearest neighbours\":{'Accuracy':knn_accuracy, 'AUC':knn_auc, 'F1 score':knn_f1},\n    \"Decision trees\":{'Accuracy':tree_accuracy, 'AUC':tree_auc, 'F1 score':tree_f1},\n    \"Support vector machine\":{'Accuracy':svm_accuracy, 'AUC':svm_auc, 'F1 score':svm_f1}\n}).T\n\n# %% [markdown]\n# ## Hyperparameter Tuning for best Classifier\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:16.200696Z\",\"iopub.execute_input\":\"2022-07-05T04:31:16.201545Z\",\"iopub.status.idle\":\"2022-07-05T04:31:16.211974Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:16.201504Z\",\"shell.execute_reply\":\"2022-07-05T04:31:16.210158Z\"}}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# %% [markdown]\n# ### Random Forest\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:16.21566Z\",\"iopub.execute_input\":\"2022-07-05T04:31:16.216468Z\",\"iopub.status.idle\":\"2022-07-05T04:31:16.227863Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:16.216415Z\",\"shell.execute_reply\":\"2022-07-05T04:31:16.226866Z\"}}\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:31:16.229325Z\",\"iopub.execute_input\":\"2022-07-05T04:31:16.230356Z\",\"iopub.status.idle\":\"2022-07-05T04:43:47.435168Z\",\"shell.execute_reply.started\":\"2022-07-05T04:31:16.230313Z\",\"shell.execute_reply\":\"2022-07-05T04:43:47.43364Z\"}}\n# Use the random grid to search for best hyperparameters\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 150, \n                               cv = 2, \n                               verbose=2, \n                               random_state=7, \n                               n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train,y_train)\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-30T04:48:06.43749Z\",\"iopub.execute_input\":\"2022-06-30T04:48:06.437926Z\",\"iopub.status.idle\":\"2022-06-30T04:48:06.445759Z\",\"shell.execute_reply.started\":\"2022-06-30T04:48:06.437887Z\",\"shell.execute_reply\":\"2022-06-30T04:48:06.444564Z\"}}\n# rf_hyper = rf_random.best_estimator_\n# rf_hyper.fit(X_train,y_train)\n# print(\"Accuracy on training set is : {}\".format(rf_hyper.score(X_train,y_train)))\n# print(\"Accuracy on validation set is : {}\".format(rf_hyper.score(X_test, y_test)))\n# rf_predict_hyper = rf_hyper.predict(X_test)\n# print(\"Accuracy of Hyper-tuned Random Forest Classifier:\",jaccard_score(y_test, rf_predict_hyper))\n# print(classification_report(y_test, rf_predict_hyper))\n\n# %% [markdown]\n# ### Gradient Boosting Classifier\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:43:47.43788Z\",\"iopub.execute_input\":\"2022-07-05T04:43:47.438255Z\",\"iopub.status.idle\":\"2022-07-05T04:43:47.449681Z\",\"shell.execute_reply.started\":\"2022-07-05T04:43:47.438221Z\",\"shell.execute_reply\":\"2022-07-05T04:43:47.448018Z\"}}\n#Number of trees\nn_estimators = [int(i) for i in np.linspace(start=100,stop=1000,num=10)]\n#Number of features to consider at every split\nmax_features = ['auto','sqrt']\n#Maximum number of levels in tree\nmax_depth = [int(i) for i in np.linspace(10, 100, num=10)]\nmax_depth.append(None)\n#Minimum number of samples required to split a node\nmin_samples_split=[2,5,10]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4]\n\n#Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:43:47.452146Z\",\"iopub.execute_input\":\"2022-07-05T04:43:47.452586Z\",\"iopub.status.idle\":\"2022-07-05T04:59:01.684462Z\",\"shell.execute_reply.started\":\"2022-07-05T04:43:47.452547Z\",\"shell.execute_reply\":\"2022-07-05T04:59:01.682901Z\"}}\ngb=GradientBoostingClassifier(random_state=0)\n#Random search of parameters, using 3 fold cross validation, \n#search across 100 different combinations\ngb_random = RandomizedSearchCV(estimator=gb, param_distributions=random_grid,\n                              n_iter=150, scoring='f1', \n                              cv=2, verbose=2, random_state=0, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\ngb_random.fit(X_train,y_train)\n\n# %% [markdown]\n# \n\n# %% [markdown]\n# #### Gradient Boosting hyper parameters\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:59:01.686128Z\",\"iopub.execute_input\":\"2022-07-05T04:59:01.686497Z\",\"iopub.status.idle\":\"2022-07-05T04:59:27.34411Z\",\"shell.execute_reply.started\":\"2022-07-05T04:59:01.686464Z\",\"shell.execute_reply\":\"2022-07-05T04:59:27.342758Z\"}}\ngb_hyper = gb_random.best_estimator_\ngb_hyper_model=gb_hyper.fit(X_train,y_train)\nprint(\"Accuracy on training set is : {}\".format(gb_hyper.score(X_train,y_train)))\nprint(\"Accuracy on validation set is : {}\".format(gb_hyper.score(X_test, y_test)))\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-29T16:47:47.393918Z\",\"iopub.status.idle\":\"2022-06-29T16:47:47.394295Z\",\"shell.execute_reply.started\":\"2022-06-29T16:47:47.39411Z\",\"shell.execute_reply\":\"2022-06-29T16:47:47.394128Z\"}}\n# #troubleshooting fit() missing 1 required positional argument: 'y'\n# gbc_predict_hyper = gb_hyper.predict(X_test)\n# gbc_acc_score = accuracy_score(y_test, gbc_predict_hyper)\n# print(\"Accuracy of Hyper-tuned Gradient Boosting Classifier:\",gbc_acc_score*100,'\\n')\n# print(classification_report(y_test, gbc_predict_hyper))\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-30T06:09:41.258049Z\",\"iopub.execute_input\":\"2022-06-30T06:09:41.258399Z\",\"iopub.status.idle\":\"2022-06-30T06:09:41.301658Z\",\"shell.execute_reply.started\":\"2022-06-30T06:09:41.258367Z\",\"shell.execute_reply\":\"2022-06-30T06:09:41.299756Z\"}}\n# lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(y_test,logistic_predict)\n# knn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(y_test,knn_predict)                                                             \n# dt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(y_test,tree_predict)\n# rf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(y_test,rf_predict)\n# rf_false_positive_rate_hyper,rf_true_positive_rate_hyper,rf_threshold_hyper = roc_curve(y_test,rf_predict_hyper)\n# gbc_false_positive_rate,gbc_true_positive_rate,gbc_threshold = roc_curve(y_test,gbc_predict)\n# gbc_false_positive_rate_hyper,gbc_true_positive_rate_hyper,gbc_threshold_hyper = roc_curve(y_test,gbc_predict_hyper)\n# svm_false_positive_rate,svm_true_positive_rate,svm_threshold = roc_curve(y_test,svm_predict)\n# #tree_false_positive_rate,tree_true_positive_rate,tree_threshold = roc_curve(y_test,tree_predict)\n# \n# \n# sns.set_style('whitegrid')\n# plt.figure(figsize=(15,8), facecolor='w')\n# plt.title('Reciever Operating Characterstic Curve')\n# #plt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\n# #plt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\n# #plt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\n# plt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\n# plt.plot(rf_false_positive_rate_hyper,rf_true_positive_rate_hyper,label='Random Forest Hyper')\n# plt.plot(gbc_false_positive_rate,gbc_true_positive_rate,label='Gradient Boosting Classifier')\n# plt.plot(gbc_false_positive_rate_hyper,gbc_true_positive_rate_hyper,label='Gradient Boosting Classifier Hyper')\n# plt.plot(svm_false_positive_rate,svm_true_positive_rate,label='Support vector')\n# plt.plot([0,1],ls='--')\n# plt.plot([0,0],[1,0],c='.5')\n# plt.plot([1,1],c='.5')\n# plt.ylabel('True positive rate')\n# plt.xlabel('False positive rate')\n# plt.legend()\n# plt.show()\n\n# %% [markdown]\n# ## Model Evaluation\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-30T11:27:46.136843Z\",\"iopub.execute_input\":\"2022-06-30T11:27:46.137292Z\",\"iopub.status.idle\":\"2022-06-30T11:27:46.315104Z\",\"shell.execute_reply.started\":\"2022-06-30T11:27:46.137257Z\",\"shell.execute_reply\":\"2022-06-30T11:27:46.313571Z\"}}\n# #without  hyperparamter tunned models predictions \n# model_ev = pd.DataFrame({'Model': ['Logistic Regression','K-Nearest Neighbour','Decision Tree',\n#                                    'Random Forest','Gradient Boosting','Support vector'], 'Accuracy': [jaccard_score(y_test, logistic_predict), jaccard_score(y_test, knn_predict), \n#                                                                                      jaccard_score(y_test, tree_predict), jaccard_score(y_test, rf_predict_hyper),jaccard_score(y_test, gbc_predict),jaccard_score(y_test, svm_predict)]})\n# model_ev\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-30T11:27:46.316366Z\",\"iopub.status.idle\":\"2022-06-30T11:27:46.31733Z\",\"shell.execute_reply.started\":\"2022-06-30T11:27:46.317086Z\",\"shell.execute_reply\":\"2022-06-30T11:27:46.317109Z\"}}\n# #with  hyperparamter tunned models predictions \n# model_ev_hyper = pd.DataFrame({'Model': ['Logistic Regression','K-Nearest Neighbour','Decision Tree',\n#                                    'Random Forest Hyper','Gradient Boosting Hyper','Support vector'], 'Accuracy': [jaccard_score(y_test, logistic_predict), jaccard_score(y_test, knn_predict), \n#                                                                                      jaccard_score(y_test, tree_predict), jaccard_score(y_test, rf_predict_hyper),jaccard_score(y_test, gbc_predict_hyper),jaccard_score(y_test, svm_predict)]})\n# model_ev_hyper\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T04:59:27.360289Z\",\"iopub.execute_input\":\"2022-07-05T04:59:27.360714Z\",\"iopub.status.idle\":\"2022-07-05T04:59:27.854939Z\",\"shell.execute_reply.started\":\"2022-07-05T04:59:27.360682Z\",\"shell.execute_reply\":\"2022-07-05T04:59:27.852897Z\"}}\n#without hyper parameters\ncolors = ['red','green','blue','gold','silver']\nplt.figure(figsize=(20,15), facecolor='w')\nplt.title(\"Barplot Representing Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Models\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-30T11:27:46.320451Z\",\"iopub.status.idle\":\"2022-06-30T11:27:46.321229Z\",\"shell.execute_reply.started\":\"2022-06-30T11:27:46.320958Z\",\"shell.execute_reply\":\"2022-06-30T11:27:46.320979Z\"}}\n# colors = ['red','green','blue','gold','silver']\n# plt.figure(figsize=(20,15), facecolor='w')\n# plt.title(\"Barplot Representing Accuracy of different models\")\n# plt.ylabel(\"Accuracy %\")\n# plt.xlabel(\"Models\")\n# plt.bar(model_ev_hyper['Model'],model_ev_hyper['Accuracy'],color = colors)\n# plt.show()\n\n# %% [markdown]\n# ## Ensembling\n# * In order to increase the accuracy of the model we use ensembling. Here we use stacking technique. We stack the 4 highest accuracy yielding models to create an ensembled model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:01:22.438423Z\",\"iopub.execute_input\":\"2022-07-05T05:01:22.438944Z\",\"iopub.status.idle\":\"2022-07-05T05:01:22.505345Z\",\"shell.execute_reply.started\":\"2022-07-05T05:01:22.438875Z\",\"shell.execute_reply\":\"2022-07-05T05:01:22.502375Z\"}}\n#Ensembling for top 4 models\nfrom mlxtend.classifier import StackingCVClassifier\n\nscv=StackingCVClassifier(classifiers=[rf_hyper, gb_hyper, knn_clf, svm_clf], meta_classifier= rf)\ntrain_x, test_x, train_y, test_y = train_test_split(X_new, y_new, test_size=0.4, random_state=1)\nstack_ens_model= scv.fit(train_x.values,train_y.values)\nscv_predict = scv.predict(test_x)\nscv_acc_score = accuracy_score(test_y, scv_predict)\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:01:22.506295Z\",\"iopub.status.idle\":\"2022-07-05T05:01:22.506745Z\",\"shell.execute_reply.started\":\"2022-07-05T05:01:22.506495Z\",\"shell.execute_reply\":\"2022-07-05T05:01:22.506513Z\"}}\n#Ensembling using the 3 models\nfrom mlxtend.classifier import StackingCVClassifier\nskip =1\n\nif skip == 0:\n    scv=StackingCVClassifier(classifiers=[rf_hyper, gb_hyper, svm_clf], meta_classifier= rf)\n    train_x, test_x, train_y, test_y = train_test_split(X_new, y_new, test_size=0.4, random_state=1)\n    stack_ens_model= scv.fit(train_x.values,train_y.values)\n    scv_predict = scv.predict(test_x)\n    scv_acc_score = accuracy_score(test_y, scv_predict)\n    print(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-29T16:47:47.409981Z\",\"iopub.status.idle\":\"2022-06-29T16:47:47.410386Z\",\"shell.execute_reply.started\":\"2022-06-29T16:47:47.410183Z\",\"shell.execute_reply\":\"2022-06-29T16:47:47.410201Z\"}}\n# #we won't split the data instead we will use the scalled data\n# #Accuracy of StackingCVClassifier: 90.91703056768559 \n# \n# scv=StackingCVClassifier(classifiers=[rf_hyper, gb_hyper, knn_clf, svm_clf], meta_classifier= rf)\n# train_x, test_x, train_y, test_y = train_test_split(X_new, y_new, test_size=0.4, random_state=1)\n# stack_ens_model= scv.fit(train_x.values,train_y.values)\n# scv_predict = scv.predict(test_x)\n# scv_acc_score = accuracy_score(test_y, scv_predict)\n# print(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-29T16:47:47.411599Z\",\"iopub.status.idle\":\"2022-06-29T16:47:47.412014Z\",\"shell.execute_reply.started\":\"2022-06-29T16:47:47.41179Z\",\"shell.execute_reply\":\"2022-06-29T16:47:47.411808Z\"}}\n# cm = confusion_matrix(test_y, scv_predict) \n# conf_matrix = pd.DataFrame(data = cm,  \n#                            columns = ['Predicted:0', 'Predicted:1'],  \n#                            index =['Actual:0', 'Actual:1']) \n# plt.figure(figsize = (8, 5)) \n# sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Blues_r\")\n# plt.show() \n#   \n# print('The details for confusion matrix is =') \n# print (classification_report(test_y, scv_predict)) \n\n# %% [markdown]\n# ## Notice\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:01:22.508758Z\",\"iopub.status.idle\":\"2022-07-05T05:01:22.509853Z\",\"shell.execute_reply.started\":\"2022-07-05T05:01:22.509577Z\",\"shell.execute_reply\":\"2022-07-05T05:01:22.5096Z\"}}\n#pd.DataFrame({'Model': ['Logistic Regression','K-Nearest Neighbour','Decision Tree','Random Forest Hyper','Gradient Boosting Hyper','Support vector'], 'Accuracy': [jaccard_score(y_test, logistic_predict), jaccard_score(y_test, knn_predict), \n                                                                                     #jaccard_score(y_test, tree_predict), jaccard_score(y_test, rf_predict_hyper),jaccard_score(y_test, gbc_predict_hyper),jaccard_score(y_test, svm_predict)]})\nscv_ens_score= jaccard_score(test_y, scv_predict)\nmodel_ev_ens = model_ev_hyper.append({\"Model\":\"Stacking Ensemble\", \"Accuracy\":scv_ens_score*100}, ignore_index=True)\nmodel_ev_ens\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:01:22.511396Z\",\"iopub.status.idle\":\"2022-07-05T05:01:22.511832Z\",\"shell.execute_reply.started\":\"2022-07-05T05:01:22.511608Z\",\"shell.execute_reply\":\"2022-07-05T05:01:22.511625Z\"}}\n## comparison\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:02:13.493489Z\",\"iopub.execute_input\":\"2022-07-05T05:02:13.493985Z\",\"iopub.status.idle\":\"2022-07-05T05:02:14.002485Z\",\"shell.execute_reply.started\":\"2022-07-05T05:02:13.49394Z\",\"shell.execute_reply\":\"2022-07-05T05:02:14.000396Z\"}}\nfig = plt.gcf()\nfig.set_size_inches(15, 15)\ntitles = ['AUC','Accuracy','F1 score']\nfor title,label in enumerate(comparison.columns):\n    plt.subplot(2,2,title+1)\n    sns.barplot(x=comparison.index, y = comparison[label], data=comparison)\n    plt.xticks(fontsize=9)\n    plt.title(titles[title])\nplt.show()\n\n# %% [markdown]\n# #### 5.5 Cross validation score of the best model\n\n# %% [markdown]\n# Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n# \n# It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n# \n# The general procedure is as follows:\n# \n# 1. Shuffle the dataset randomly.\n# 2. Split the dataset into k groups\n# 3. For each unique group:\n#     *         Take the group as a hold out or test data set\n#     *         Take the remaining groups as a training data set\n#     *         Fit a model on the training set and evaluate it on the test set\n#     *         Retain the evaluation score and discard the model\n# 4. Summarize the skill of the model using the sample of model evaluation scores\n# \n# Check the full reference [here](https://machinelearningmastery.com/k-fold-cross-validation/)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-07-05T05:02:14.005116Z\",\"iopub.execute_input\":\"2022-07-05T05:02:14.00557Z\",\"iopub.status.idle\":\"2022-07-05T05:02:14.012385Z\",\"shell.execute_reply.started\":\"2022-07-05T05:02:14.005533Z\",\"shell.execute_reply\":\"2022-07-05T05:02:14.010386Z\"}}\n from sklearn.model_selection import cross_val_score\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-29T16:47:47.421728Z\",\"iopub.status.idle\":\"2022-06-29T16:47:47.422154Z\",\"shell.execute_reply.started\":\"2022-06-29T16:47:47.421951Z\",\"shell.execute_reply\":\"2022-06-29T16:47:47.421969Z\"}}\n# cv_results = cross_val_score(svm_clf, X, y, cv=5) \n# \n# print (f\"Cross-validated scores {cv_results}\")\n# print(f\"The Cross Validation accuracy is: {round(cv_results.mean() * 100,2)}%\")\n\n# %% [raw] {\"execution\":{\"iopub.status.busy\":\"2022-06-29T16:47:47.423958Z\",\"iopub.status.idle\":\"2022-06-29T16:47:47.424341Z\",\"shell.execute_reply.started\":\"2022-06-29T16:47:47.424154Z\",\"shell.execute_reply\":\"2022-06-29T16:47:47.424171Z\"}}\n# #cross validation for the ensumble\n# cv_results_ens = cross_val_score(scv, X, y, cv=5) \n# \n# print (f\"Cross-validated scores {cv_results_ens}\")\n# print(f\"The Cross Validation accuracy is: {round(cv_results_ens.mean() * 100,2)}%\")\n\n# %% [markdown]\n# ### 6. Conclusion\n\n# %% [markdown]\n# 1. The most important features in predicting the ten year risk of developing CHD were age and systolic blood pressure\n# 2. The Support vector machine with the radial kernel was the best performing model in terms of accuracy and the F1 score. Its high AUC shows that it has a high true positive rate.\n# 3. Balancing the dataset by using the SMOTE technique helped in improving the models' sensitivity, this is when compared to the performance metrics of other models on different notebooks on the same dataset\n# 4. With more data(especially that of the minority class) better models can be built\n\n# %% [markdown]\n# **Closing statements**\n# 1. If you have found this notebook to be useful please upvote it\n# 2. If you have any queries or additions please leave them in the comments\n# 3. Feel free to use any part of this notebook in your personal work\n# \n\n# %% [markdown]\n#","metadata":{"_uuid":"8342bb7c-d6ad-4205-9745-437cdc38ea7e","_cell_guid":"20c88614-7593-414d-ad3d-5b55e92b1e79","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}